"""
Adapted from spacy-huggingface-hub: spacy_huggingface_hub/push.py
to allow for a more detailed model card.


"""

import json
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import typer
import yaml

TOKEN_CLASSIFICATION_COMPONENTS = ["ner", "tagger", "morphologizer"]
TEXT_CLASSIFICATION_COMPONENTS = ["textcat", "textcat_multilabel"]


def _create_model_card(repo_name: str, repo_dir: Path) -> Dict[str, Any]:
    meta_path = repo_dir / "meta.json"
    with meta_path.open("r", encoding="utf-8") as f:
        data = json.load(f)
    lang = data["lang"] if data["lang"] != "xx" else "multilingual"

    lic = data.get("license", "").replace(" ", "-").lower()
    # HF accepts gpl-3.0 directly
    lic = lic.replace("gnu-", "")

    tags = ["spacy", "dacy", "danish"]
    for component in data["components"]:
        if (
            component in TOKEN_CLASSIFICATION_COMPONENTS
            and "token-classification" not in tags
        ):
            tags.append("token-classification")
        if (
            component in TEXT_CLASSIFICATION_COMPONENTS
            and "text-classification" not in tags
        ):
            tags.append("text-classification")

        if component == "ner":
            tags.append("named entity recognition")

        if component == "tagger":
            tags.append("pos tagging")
        if component == "trainable_lemmatizer":
            tags.append("lemmatization")
        if component == "parser":
            tags.append("dependency parsing")
        if component == "coref":
            tags.append("coreference resolution")
        if component == "entity_linker":
            tags.append("named entity linking")
            tags.append("named entity disambiguation")
        if component == "morphologizer":
            tags.append("morphological analysis")

    metadata = _insert_values_as_list({}, "tags", tags)
    metadata = _insert_values_as_list(metadata, "language", lang)
    metadata = _insert_value(metadata, "license", lic)
    metadata["model-index"] = _create_model_index(repo_name, data["performance"])
    metadata["library_name"] = "spacy"
    metadata["datasets"] = ["universal_dependencies"]
    for component in data["components"]:
        if component == "ner":
            metadata["datasets"].append("dane")
        if component == "coref":
            metadata["datasets"].append("alexandrainst/dacoref")

    metadata["metrics"] = ["accuracy"]
    metadata = yaml.dump(metadata, sort_keys=False)
    metadata_section = f"---\n{metadata}---\n"

    # Read README generated by package
    readme_path = repo_dir / "README.md"
    readme = ""
    if readme_path.exists():
        with readme_path.open("r", encoding="utf8") as f:
            readme = f.read()
    with readme_path.open("w", encoding="utf-8") as f:
        f.write(metadata_section)
        f.write(readme)
    print(f"Updated {readme_path}")
    return metadata


def _insert_value(
    metadata: Dict[str, Any],
    name: str,
    value: Optional[Any],
) -> Dict[str, Any]:
    if value is None or value == "":
        return metadata
    metadata[name] = value
    return metadata


def _insert_values_as_list(
    metadata: Dict[str, Any],
    name: str,
    values: Optional[Any],
) -> Dict[str, List[Any]]:
    if values is None:
        return metadata
    if isinstance(values, str):
        values = [values]
    if len(values) == 0:
        return metadata
    metadata[name] = list(values)
    return metadata


def _create_metric(name: str, t: str, value: float) -> Dict[str, Union[str, float]]:
    return {"name": name, "type": t, "value": value}


def _create_p_r_f_list(
    metric_name: str,
    precision: float,
    recall: float,
    f_score: float,
) -> List[Dict[str, Union[str, float]]]:
    precision = _create_metric(f"{metric_name} Precision", "precision", precision)  # type: ignore
    recall = _create_metric(f"{metric_name} Recall", "recall", recall)  # type: ignore
    f_score = _create_metric(f"{metric_name} F Score", "f_score", f_score)  # type: ignore
    return [precision, recall, f_score]  # type: ignore


def _create_model_index(repo_name: str, data: Dict[str, Any]) -> List[Dict[str, Any]]:
    # TODO: add some more metrics here
    model_index = {"name": repo_name}
    results = []
    if "ents_p" in data:
        results.append(
            {
                "task": {"name": "NER", "type": "token-classification"},
                "metrics": _create_p_r_f_list(
                    "NER",
                    data["ents_p"],
                    data["ents_r"],
                    data["ents_f"],
                ),
                "dataset": {"name": "DaNE", "split": "test", "type": "dane"},
            },
        )
    if "tag_acc" in data:
        results.append(
            {
                "task": {"name": "TAG", "type": "token-classification"},
                "metrics": [
                    _create_metric("TAG (XPOS) Accuracy", "accuracy", data["tag_acc"]),
                ],
                "dataset": {
                    "name": "UD Danish DDT",
                    "split": "test",
                    "type": "universal_dependencies",
                    "config": "da_ddt",
                },
            },
        )
    if "pos_acc" in data:
        results.append(
            {
                "task": {"name": "POS", "type": "token-classification"},
                "metrics": [
                    _create_metric("POS (UPOS) Accuracy", "accuracy", data["pos_acc"]),
                ],
                "dataset": {
                    "name": "UD Danish DDT",
                    "split": "test",
                    "type": "universal_dependencies",
                    "config": "da_ddt",
                },
            },
        )
    if "morph_acc" in data:
        results.append(
            {
                "task": {"name": "MORPH", "type": "token-classification"},
                "metrics": [
                    _create_metric(
                        "Morph (UFeats) Accuracy",
                        "accuracy",
                        data["morph_acc"],
                    ),
                ],
                "dataset": {
                    "name": "UD Danish DDT",
                    "split": "test",
                    "type": "universal_dependencies",
                    "config": "da_ddt",
                },
            },
        )
    if "lemma_acc" in data:
        results.append(
            {
                "task": {"name": "LEMMA", "type": "token-classification"},
                "metrics": [
                    _create_metric("Lemma Accuracy", "accuracy", data["lemma_acc"]),
                ],
                "dataset": {
                    "name": "UD Danish DDT",
                    "split": "test",
                    "type": "universal_dependencies",
                    "config": "da_ddt",
                },
            },
        )
    if "dep_uas" in data:
        results.append(
            {
                "task": {
                    "name": "UNLABELED_DEPENDENCIES",
                    "type": "token-classification",
                },
                "metrics": [
                    _create_metric(
                        "Unlabeled Attachment Score (UAS)",
                        "f_score",
                        data["dep_uas"],
                    ),
                ],
                "dataset": {
                    "name": "UD Danish DDT",
                    "split": "test",
                    "type": "universal_dependencies",
                    "config": "da_ddt",
                },
            },
        )
    if "dep_las" in data:
        results.append(
            {
                "task": {
                    "name": "LABELED_DEPENDENCIES",
                    "type": "token-classification",
                },
                "metrics": [
                    _create_metric(
                        "Labeled Attachment Score (LAS)",
                        "f_score",
                        data["dep_las"],
                    ),
                ],
                "dataset": {
                    "name": "UD Danish DDT",
                    "split": "test",
                    "type": "universal_dependencies",
                    "config": "da_ddt",
                },
            },
        )
    if "sents_p" in data:
        results.append(
            {
                "task": {"name": "SENTS", "type": "token-classification"},
                "metrics": [
                    _create_metric("Sentences F-Score", "f_score", data["sents_f"]),
                ],
                "dataset": {
                    "name": "UD Danish DDT",
                    "split": "test",
                    "type": "universal_dependencies",
                    "config": "da_ddt",
                },
            },
        )
    if "coref_lea_f1" in data:
        results.append(
            {
                "task": {
                    "name": "coreference-resolution",
                    "type": "coreference-resolution",
                },
                "metrics": [
                    _create_metric("LEA", "f_score", data["coref_lea_f1"]),
                ],
                "dataset": {
                    "name": "DaCoref",
                    "type": "alexandrainst/dacoref",
                    "split": "custom",
                },
            },
        )
    if "nel_micro_f" in data:
        results.append(
            {
                "task": {
                    "name": "coreference-resolution",
                    "type": "coreference-resolution",
                },
                "metrics": _create_p_r_f_list(
                    "Named entity Linking",
                    data["nel_micro_p"],
                    data["nel_micro_r"],
                    data["nel_micro_f"],
                ),
                "dataset": {
                    "name": "DaNED",
                    "type": "named-entity-linking",
                    "split": "custom",
                },
            },
        )
    model_index["results"] = results  # type: ignore
    return [model_index]


def main(repo_path: Path) -> None:
    repo_name = repo_path.name
    repo_dir = repo_path
    _create_model_card(
        repo_name=repo_name,
        repo_dir=repo_dir,
    )


if __name__ == "__main__":
    typer.run(main)
