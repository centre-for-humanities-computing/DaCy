[paths]
train = null
dev = null
init_tok2vec = null
vectors = null
model_source = "replace me!"

[system]
gpu_allocator = "pytorch"
seed = 0

[nlp]
lang = "da"
pipeline = ["transformer", "tagger","morphologizer","trainable_lemmatizer","parser","ner", "coref", "span_resolver", "span_cleaner", "entity_linker"]
batch_size = 512
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {"@tokenizers":"spacy.Tokenizer.v1"}

[components]

[components.entity_linker]
source = ${paths.model_source}

[components.span_resolver]
source = "replace me!"

[components.coref]
source = ${paths.model_source}

[components.morphologizer]
source = ${paths.model_source}

[components.ner]
source = ${paths.model_source}

[components.parser]
source = ${paths.model_source}

[components.tagger]
source = ${paths.model_source}

[components.trainable_lemmatizer]
source = ${paths.model_source}

[components.transformer]
source = ${paths.model_source}

[components.span_cleaner]
factory = "experimental_span_cleaner"
prefix = "coref_head_clusters"
