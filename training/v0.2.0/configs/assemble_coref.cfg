[paths]
train = null
dev = null
init_tok2vec = null
vectors = null

[system]
gpu_allocator = "pytorch"
seed = 0

[nlp]
lang = "da"
pipeline = ["sentencizer", "transformer", "coref", "span_resolver", "span_cleaner"]
batch_size = 2
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {"@tokenizers":"spacy.Tokenizer.v1"}

[components]

[components.sentencizer]
factory = "sentencizer"
punct_chars = null

[components.span_resolver]
source = "replace me!"

[components.coref]
source = "replace me!"

[components.transformer]
source = "replace me!"

[components.span_cleaner]
factory = "experimental_span_cleaner"
prefix = "coref_head_clusters"
