{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/KennethEnevoldsen/DaCy/blob/master/docs/performance.ner.ipynb)\n",
    "\n",
    "\n",
    "This page examines the performance of competing models for Danish named entity recognition over multiple datasets. Performance is not limited to \n",
    "accuracy, but also includes domain generalization, biases and robustness. This page is also a notebook, which open and replicate the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State-of-the-Art comparison\n",
    "To our knowledge there exists three datasets for Danish named entity recognition;\n",
    "\n",
    "1) DaNE {cite}`hvingelby2020dane`, which uses the simple annotation scheme of CoNLL 2003 {cite}`missing` with the entities; *person*, *location*, *organization*, and *miscellaneus*.\n",
    "2) DANSK {cite}`missing`, which uses the extensive annotation scheme similar to that of OntoNotes 5.0 {cite}`missing` including more that 16 entity types.\n",
    "3) and DAN+ {cite}`missing`, which also uses the annotation scheme of CoNLL 2003, but allows for nested entities for instance *Aarhus Universitet*, where *Aarhus* is a location and *Aarhus Universitet* is an organization.\n",
    "\n",
    "In this comparison we will be examing performance on DaNE and DANSK, but as no known models have been trained on Danish nested entities, we will not be comparing performance on DAN+.\n",
    "\n",
    "\n",
    "```{admonition} Measuring Performance\n",
    "Typically when measuring performance on these benchmark it is normal to feed the model the gold standard tokens. While this allows for easier comparisons of modules and architectures, it inflates the performance metrics. Further, it does not proberly reflect what you are really interested in:\n",
    "*the performance you can expect when you apply the model to data of a similar type*. Therefore we estimate the model is given no prior knowledge of the data, and only the raw text is fed to the model. Thus the performance metrics might be slightly different compared to e.g. DaNLP.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DaNE: Simple Named Entity Recognition\n",
    "As already stated DaNE uses an extraction from the CoNLL 2003 dataset, which is as follows {cite}`hvingelby2020dane`:\n",
    "\n",
    "\n",
    "| Entity | Description |\n",
    "|--------------|-------------|\n",
    "| LOC          | includes locations like cities, roads and mountains, as well as both public and commercial places like specific buildings or meeting points, but also abstract places. |\n",
    "| PERSON | consists of names of people, fictional characters, and animals. The names includes aliases. |\n",
    "| ORG | can be summarized as all sorts of organizations and collections of people, ranging from companies, brands, political movements, governmental bodies and clubs. |\n",
    "| MISC | is a broad category of e.g. events, languages, titles and religions, but this tag also includes words derived from one of the four tags as well as words for which one part is from one of the three other tags. |\n",
    "\n",
    "Here is an example from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">To kendte \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    russiske\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " historikere \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Andronik Mirganjan\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " og \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Igor Klamkin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " tror ikke, at \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Rusland\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " kan udvikles uden en &quot;jernnæve&quot;.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy \n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n",
    "text = \"\"\"To kendte russiske historikere Andronik Mirganjan og Igor Klamkin tror ikke, at Rusland kan udvikles uden en \"jernnæve\".\"\"\"\n",
    "nlp = spacy.blank(\"da\")\n",
    "doc = nlp(text)\n",
    "doc.ents = [ # type: ignore\n",
    "    Span(doc, 2, 3, label=\"MISC\"),\n",
    "    Span(doc, 4, 6, label=\"PERSON\"),\n",
    "    Span(doc, 7, 9, label=\"PERSON\"),\n",
    "    Span(doc, 13, 14, label=\"LOC\"),\n",
    "]\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the performance of Danish language processing pipelines scored on the DaNE test set. The best scores in each category are highlighted with bold and the second best is underlined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from performance_testing_utils.ner_sota_utils import apply_models, MDL_GETTER_DICT\n",
    "from dacy.datasets import dane\n",
    "from spacy.training import Example\n",
    "\n",
    "\n",
    "def apply_models(\n",
    "def apply_models(\n",
    "    mdl_name: str, nlp: Language, examples: list[Example]\n",
    ") -> pd.DataFrame:\n",
    "    texts = [example.reference.text for example in examples]\n",
    "    docs = nlp.pipe(texts)\n",
    "    for doc, example in zip(docs, examples):\n",
    "        example.predicted = doc\n",
    "    return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model: saattrupdan/nbailab-base-ner-scandi\n",
      "- Already exists, loading in dataframe\n",
      "Running model: da_dacy_large_trf-0.2.0\n",
      "- Already exists, loading in dataframe\n",
      "Running model: da_dacy_medium_trf-0.2.0\n",
      "- Already exists, loading in dataframe\n",
      "Running model: da_dacy_small_trf-0.2.0\n",
      "- Already exists, loading in dataframe\n",
      "Running model: alexandrainst/da-ner-base\n",
      "- Already exists, loading in dataframe\n",
      "Running model: da_core_news_trf-3.5.0\n",
      "- Already exists, loading in dataframe\n",
      "Running model: da_core_news_lg-3.5.0\n",
      "- Already exists, loading in dataframe\n",
      "Running model: da_core_news_md-3.5.0\n",
      "- Already exists, loading in dataframe\n",
      "Running model: da_core_news_sm-3.5.0\n",
      "- Already exists, loading in dataframe\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from performance_testing_utils.ner_sota_utils import apply_models, MDL_GETTER_DICT, create_table\n",
    "from dacy.datasets import dane\n",
    "from spacy.training import Example\n",
    "\n",
    "force=False\n",
    "save_folder = Path(\"performance_tables/ner\")\n",
    "save_folder.mkdir(exist_ok=True, parents=True)\n",
    "nlp = spacy.blank(\"da\")\n",
    "examples: list[Example]= list(dane(splits = [\"test\"])(nlp)) # type : ignore\n",
    "\n",
    "\n",
    "tables = []\n",
    "for model_name, getter in MDL_GETTER_DICT.items():\n",
    "    print(\"Running model:\", model_name)\n",
    "    model_name_ = model_name.replace(\"/\", \"_\")\n",
    "    save_path = save_folder / f\"{model_name_}_sota_dane.csv\"\n",
    "    if not save_path.exists() or force:\n",
    "        nlp = getter()\n",
    "        result_df = apply_models(model_name, nlp, examples)\n",
    "        result_df.to_csv(save_path, index=False)\n",
    "    else:\n",
    "        print(\"- Already exists, loading in dataframe\")\n",
    "        result_df = pd.read_csv(save_path)\n",
    "    tables.append(result_df)\n",
    "\n",
    "df = pd.concat(tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5d43f .level0 {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_5d43f .col_heading {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_5d43f_row0_col0, #T_5d43f_row1_col0, #T_5d43f_row2_col0, #T_5d43f_row3_col0, #T_5d43f_row4_col0, #T_5d43f_row5_col0, #T_5d43f_row6_col0, #T_5d43f_row7_col0, #T_5d43f_row8_col0 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_5d43f_row0_col1, #T_5d43f_row0_col3, #T_5d43f_row0_col5, #T_5d43f_row1_col2, #T_5d43f_row1_col4 {\n",
       "  font-weight: bold;\n",
       "  text-align: right;\n",
       "}\n",
       "#T_5d43f_row0_col2, #T_5d43f_row2_col1, #T_5d43f_row2_col3, #T_5d43f_row2_col4, #T_5d43f_row2_col5, #T_5d43f_row3_col1, #T_5d43f_row3_col2, #T_5d43f_row3_col3, #T_5d43f_row3_col4, #T_5d43f_row3_col5, #T_5d43f_row4_col1, #T_5d43f_row4_col2, #T_5d43f_row4_col3, #T_5d43f_row4_col4, #T_5d43f_row4_col5, #T_5d43f_row5_col1, #T_5d43f_row5_col2, #T_5d43f_row5_col3, #T_5d43f_row5_col4, #T_5d43f_row5_col5, #T_5d43f_row6_col1, #T_5d43f_row6_col2, #T_5d43f_row6_col3, #T_5d43f_row6_col4, #T_5d43f_row6_col5, #T_5d43f_row7_col1, #T_5d43f_row7_col2, #T_5d43f_row7_col3, #T_5d43f_row7_col4, #T_5d43f_row7_col5, #T_5d43f_row8_col1, #T_5d43f_row8_col2, #T_5d43f_row8_col3, #T_5d43f_row8_col4, #T_5d43f_row8_col5 {\n",
       "  text-align: right;\n",
       "}\n",
       "#T_5d43f_row0_col4, #T_5d43f_row1_col1, #T_5d43f_row1_col3, #T_5d43f_row1_col5, #T_5d43f_row2_col2 {\n",
       "  text-decoration: underline;\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5d43f\">\n",
       "  <caption>F1 score with 95% confidence interval calculated using bootstrapping with 100 samples.</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_5d43f_level0_col0\" class=\"col_heading level0 col0\" ></th>\n",
       "      <th id=\"T_5d43f_level0_col1\" class=\"col_heading level0 col1\" colspan=\"5\">F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5d43f_level1_col0\" class=\"col_heading level1 col0\" >Models</th>\n",
       "      <th id=\"T_5d43f_level1_col1\" class=\"col_heading level1 col1\" >Average</th>\n",
       "      <th id=\"T_5d43f_level1_col2\" class=\"col_heading level1 col2\" >Misc.</th>\n",
       "      <th id=\"T_5d43f_level1_col3\" class=\"col_heading level1 col3\" >Organization</th>\n",
       "      <th id=\"T_5d43f_level1_col4\" class=\"col_heading level1 col4\" >Location</th>\n",
       "      <th id=\"T_5d43f_level1_col5\" class=\"col_heading level1 col5\" >Person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_5d43f_row0_col0\" class=\"data row0 col0\" >saattrupdan/nbailab-base-ner-scandi</td>\n",
       "      <td id=\"T_5d43f_row0_col1\" class=\"data row0 col1\" >86.2 (82.3, 89.3)</td>\n",
       "      <td id=\"T_5d43f_row0_col2\" class=\"data row0 col2\" >78.8 (72.6, 87.3)</td>\n",
       "      <td id=\"T_5d43f_row0_col3\" class=\"data row0 col3\" >80.3 (74.7, 85.9)</td>\n",
       "      <td id=\"T_5d43f_row0_col4\" class=\"data row0 col4\" >88.3 (82.7, 93.4)</td>\n",
       "      <td id=\"T_5d43f_row0_col5\" class=\"data row0 col5\" >95.1 (92.0, 97.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5d43f_row1_col0\" class=\"data row1 col0\" >da_dacy_large_trf-0.2.0</td>\n",
       "      <td id=\"T_5d43f_row1_col1\" class=\"data row1 col1\" >85.4 (81.6, 88.7)</td>\n",
       "      <td id=\"T_5d43f_row1_col2\" class=\"data row1 col2\" >79.7 (71.7, 85.3)</td>\n",
       "      <td id=\"T_5d43f_row1_col3\" class=\"data row1 col3\" >78.9 (73.1, 85.2)</td>\n",
       "      <td id=\"T_5d43f_row1_col4\" class=\"data row1 col4\" >89.4 (83.5, 94.4)</td>\n",
       "      <td id=\"T_5d43f_row1_col5\" class=\"data row1 col5\" >92.6 (89.6, 95.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5d43f_row2_col0\" class=\"data row2 col0\" >da_dacy_medium_trf-0.2.0</td>\n",
       "      <td id=\"T_5d43f_row2_col1\" class=\"data row2 col1\" >84.7 (80.5, 88.7)</td>\n",
       "      <td id=\"T_5d43f_row2_col2\" class=\"data row2 col2\" >79.0 (72.5, 84.9)</td>\n",
       "      <td id=\"T_5d43f_row2_col3\" class=\"data row2 col3\" >78.6 (70.8, 84.9)</td>\n",
       "      <td id=\"T_5d43f_row2_col4\" class=\"data row2 col4\" >86.4 (79.4, 92.6)</td>\n",
       "      <td id=\"T_5d43f_row2_col5\" class=\"data row2 col5\" >92.3 (88.8, 95.6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5d43f_row3_col0\" class=\"data row3 col0\" >da_dacy_small_trf-0.2.0</td>\n",
       "      <td id=\"T_5d43f_row3_col1\" class=\"data row3 col1\" >82.4 (79.5, 85.1)</td>\n",
       "      <td id=\"T_5d43f_row3_col2\" class=\"data row3 col2\" >75.1 (68.6, 81.1)</td>\n",
       "      <td id=\"T_5d43f_row3_col3\" class=\"data row3 col3\" >75.2 (71.3, 79.9)</td>\n",
       "      <td id=\"T_5d43f_row3_col4\" class=\"data row3 col4\" >83.8 (78.2, 88.4)</td>\n",
       "      <td id=\"T_5d43f_row3_col5\" class=\"data row3 col5\" >92.2 (90.0, 95.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5d43f_row4_col0\" class=\"data row4 col0\" >alexandrainst/da-ner-base</td>\n",
       "      <td id=\"T_5d43f_row4_col1\" class=\"data row4 col1\" >70.4 (66.6, 74.1)</td>\n",
       "      <td id=\"T_5d43f_row4_col2\" class=\"data row4 col2\" > </td>\n",
       "      <td id=\"T_5d43f_row4_col3\" class=\"data row4 col3\" >64.7 (56.0, 71.0)</td>\n",
       "      <td id=\"T_5d43f_row4_col4\" class=\"data row4 col4\" >84.9 (77.3, 91.1)</td>\n",
       "      <td id=\"T_5d43f_row4_col5\" class=\"data row4 col5\" >90.0 (87.0, 93.1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5d43f_row5_col0\" class=\"data row5 col0\" >da_core_news_trf-3.5.0</td>\n",
       "      <td id=\"T_5d43f_row5_col1\" class=\"data row5 col1\" >78.7 (74.2, 82.1)</td>\n",
       "      <td id=\"T_5d43f_row5_col2\" class=\"data row5 col2\" >69.0 (59.8, 75.8)</td>\n",
       "      <td id=\"T_5d43f_row5_col3\" class=\"data row5 col3\" >67.8 (60.1, 73.8)</td>\n",
       "      <td id=\"T_5d43f_row5_col4\" class=\"data row5 col4\" >81.8 (72.7, 88.8)</td>\n",
       "      <td id=\"T_5d43f_row5_col5\" class=\"data row5 col5\" >91.3 (88.1, 94.8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5d43f_row6_col0\" class=\"data row6 col0\" >da_core_news_lg-3.5.0</td>\n",
       "      <td id=\"T_5d43f_row6_col1\" class=\"data row6 col1\" >74.7 (71.2, 77.7)</td>\n",
       "      <td id=\"T_5d43f_row6_col2\" class=\"data row6 col2\" >64.2 (54.9, 72.2)</td>\n",
       "      <td id=\"T_5d43f_row6_col3\" class=\"data row6 col3\" >63.1 (55.7, 70.1)</td>\n",
       "      <td id=\"T_5d43f_row6_col4\" class=\"data row6 col4\" >81.6 (75.6, 88.5)</td>\n",
       "      <td id=\"T_5d43f_row6_col5\" class=\"data row6 col5\" >85.6 (80.5, 89.6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5d43f_row7_col0\" class=\"data row7 col0\" >da_core_news_md-3.5.0</td>\n",
       "      <td id=\"T_5d43f_row7_col1\" class=\"data row7 col1\" >70.9 (67.1, 74.2)</td>\n",
       "      <td id=\"T_5d43f_row7_col2\" class=\"data row7 col2\" >61.6 (52.8, 69.8)</td>\n",
       "      <td id=\"T_5d43f_row7_col3\" class=\"data row7 col3\" >58.5 (51.8, 65.8)</td>\n",
       "      <td id=\"T_5d43f_row7_col4\" class=\"data row7 col4\" >75.6 (67.7, 82.1)</td>\n",
       "      <td id=\"T_5d43f_row7_col5\" class=\"data row7 col5\" >82.8 (79.1, 86.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5d43f_row8_col0\" class=\"data row8 col0\" >da_core_news_sm-3.5.0</td>\n",
       "      <td id=\"T_5d43f_row8_col1\" class=\"data row8 col1\" >64.1 (60.0, 67.5)</td>\n",
       "      <td id=\"T_5d43f_row8_col2\" class=\"data row8 col2\" >58.1 (50.4, 66.5)</td>\n",
       "      <td id=\"T_5d43f_row8_col3\" class=\"data row8 col3\" >49.0 (42.0, 56.7)</td>\n",
       "      <td id=\"T_5d43f_row8_col4\" class=\"data row8 col4\" >61.1 (53.3, 69.5)</td>\n",
       "      <td id=\"T_5d43f_row8_col5\" class=\"data row8 col5\" >79.8 (74.7, 83.9)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16a0825f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_table(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Note that `saattrupdan/nbailab-base-ner-scandi` is available in DaCy using `nlp.add_pipe(\"dacy/ner\")`\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} You are missing a model\n",
    ":note:\n",
    "\n",
    "These tables are continually updated and thus we try to limit the number of models to only the most relevant Danish models. Therefore models like Polyglot with strict requirements and consistently worse performance are excluded. If you want to see a specific model, please open an issue on GitHub.\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DANSK: Fine-grained Named Entity Recognition\n",
    "\n",
    "DANSK is annotated from the Danish Gigaword Corpus {cite}`missing` and a wide variety of domains including conversational, legal, news, social media, web content,  wiki's and Books. Dansk follows includes the following labels:\n",
    "\n",
    "\n",
    "|  Entity        |             Description                                         |\n",
    "| -------- | ---------------------------------------------------- |\n",
    "| PERSON   | People, including fictional                          |\n",
    "| NORP     | Nationalities or religious or political groups       |\n",
    "| FACILITY | Building, airports, highways, bridges, etc.          |\n",
    "| ORGANIZATION | Companies, agencies, institutions, etc.              |\n",
    "| GPE      | Countries, cities, states.                           |\n",
    "| LOCATION | Non-GPE locations, mountain ranges, bodies of water  |\n",
    "| PRODUCT  | Vehicles, weapons, foods, etc. (not services)        |\n",
    "| EVENT    | Named hurricanes, battles, wars, sports events, etc. |\n",
    "| WORK OF ART | Titles of books, songs, etc.                         |\n",
    "| LAW      | Named documents made into laws                       |\n",
    "| LANGUAGE | Any named language                                   |\n",
    "\n",
    "As well as annotation for the following concepts:\n",
    "\n",
    "|   Entity       |   Description                                         |\n",
    "| -------- | ------------------------------------------- |\n",
    "| DATE     | Absolute or relative dates or periods       |\n",
    "| TIME     | Times smaller than a day                    |\n",
    "| PERCENT  | Percentage (including \"*\"%)                |\n",
    "| MONEY    | Monetary values, including unit             |\n",
    "| QUANTITY | Measurements, as of weight or distance      |\n",
    "| ORDINAL  | \"first\", \"second\"                           |\n",
    "| CARDINAL | Numerals that do no fall under another type |\n",
    "\n",
    "\n",
    "We have here opted to create an interactive chart over a table as with the number of labels it quickly becomes unruly. The chart is interactive and you can select the label you want to compare the models on. You can also hover over the dots the see the exact values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/au561649/.cache/huggingface/datasets/chcaa___parquet/chcaa--DANSK-ec592bb9b8d7fe08/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/Users/au561649/.cache/huggingface/datasets/chcaa___parquet/chcaa--DANSK-ec592bb9b8d7fe08/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/Users/au561649/.cache/huggingface/datasets/chcaa___parquet/chcaa--DANSK-ec592bb9b8d7fe08/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model: da_dacy_large_ner_fine_grained-0.1.0\n",
      "- Already exists, loading in dataframe\n",
      "Running model: da_dacy_medium_ner_fine_grained-0.1.0\n",
      "- Already exists, loading in dataframe\n",
      "Running model: da_dacy_small_ner_fine_grained-0.1.0\n",
      "- Already exists, loading in dataframe\n"
     ]
    }
   ],
   "source": [
    "from performance_testing_utils.ner_sota_utils import apply_models, MDL_FINE_GETTER_DICT, dansk, create_dansk_viz\n",
    "\n",
    "force=False\n",
    "train, dev, test = dansk()\n",
    "examples = [Example(x, x) for x in test]\n",
    "\n",
    "tables = []\n",
    "for model_name, getter in MDL_FINE_GETTER_DICT.items():\n",
    "    print(\"Running model:\", model_name)\n",
    "    model_name_ = model_name.replace(\"/\", \"_\")\n",
    "    save_path = save_folder / f\"{model_name_}_sota_dansk.csv\"\n",
    "    if not save_path.exists() or force:\n",
    "        nlp = getter()\n",
    "        result_df = apply_models(model_name, nlp, examples, decimals=1)\n",
    "        result_df.to_csv(save_path, index=False)\n",
    "    else:\n",
    "        print(\"- Already exists, loading in dataframe\")\n",
    "    result_df = pd.read_csv(save_path)\n",
    "    tables.append(result_df)\n",
    "\n",
    "df = pd.concat(tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m create_dansk_viz(df)\n",
      "File \u001b[0;32m~/Github/DaCy/docs/performance_testing_utils/ner_sota_utils.py:106\u001b[0m, in \u001b[0;36mcreate_dansk_viz\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m# Convert the score value to a float\u001b[39;00m\n\u001b[1;32m    105\u001b[0m plot_df[\u001b[39m\"\u001b[39m\u001b[39mF1\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m plot_df[\u001b[39m\"\u001b[39m\u001b[39mF1 string\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mfloat\u001b[39m(x\u001b[39m.\u001b[39msplit()[\u001b[39m0\u001b[39m]) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mfloat\u001b[39m) \u001b[39melse\u001b[39;00m x)\n\u001b[0;32m--> 106\u001b[0m plot_df[\u001b[39m\"\u001b[39m\u001b[39mCI Lower\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m plot_df[\u001b[39m\"\u001b[39;49m\u001b[39mF1 string\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m    107\u001b[0m     \u001b[39mlambda\u001b[39;49;00m x: \u001b[39mfloat\u001b[39;49m(x\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m(\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    108\u001b[0m )\n\u001b[1;32m    109\u001b[0m plot_df[\u001b[39m\"\u001b[39m\u001b[39mCI Upper\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m plot_df[\u001b[39m\"\u001b[39m\u001b[39mF1 string\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\n\u001b[1;32m    110\u001b[0m     \u001b[39mlambda\u001b[39;00m x: \u001b[39mfloat\u001b[39m(x\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m])\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    113\u001b[0m selection \u001b[39m=\u001b[39m alt\u001b[39m.\u001b[39mselection_point(\n\u001b[1;32m    114\u001b[0m     fields\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mLabel\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    115\u001b[0m     bind\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlegend\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m     value\u001b[39m=\u001b[39m[{\u001b[39m\"\u001b[39m\u001b[39mLabel\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mAverage\u001b[39m\u001b[39m\"\u001b[39m}],\n\u001b[1;32m    117\u001b[0m )\n",
      "File \u001b[0;32m~/.virtualenvs/dacy/lib/python3.10/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/.virtualenvs/dacy/lib/python3.10/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/.virtualenvs/dacy/lib/python3.10/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1175\u001b[0m             values,\n\u001b[1;32m   1176\u001b[0m             f,\n\u001b[1;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1178\u001b[0m         )\n\u001b[1;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.virtualenvs/dacy/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Github/DaCy/docs/performance_testing_utils/ner_sota_utils.py:107\u001b[0m, in \u001b[0;36mcreate_dansk_viz.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m# Convert the score value to a float\u001b[39;00m\n\u001b[1;32m    105\u001b[0m plot_df[\u001b[39m\"\u001b[39m\u001b[39mF1\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m plot_df[\u001b[39m\"\u001b[39m\u001b[39mF1 string\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mfloat\u001b[39m(x\u001b[39m.\u001b[39msplit()[\u001b[39m0\u001b[39m]) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mfloat\u001b[39m) \u001b[39melse\u001b[39;00m x)\n\u001b[1;32m    106\u001b[0m plot_df[\u001b[39m\"\u001b[39m\u001b[39mCI Lower\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m plot_df[\u001b[39m\"\u001b[39m\u001b[39mF1 string\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mlambda\u001b[39;00m x: \u001b[39mfloat\u001b[39m(x\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m])\n\u001b[1;32m    108\u001b[0m )\n\u001b[1;32m    109\u001b[0m plot_df[\u001b[39m\"\u001b[39m\u001b[39mCI Upper\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m plot_df[\u001b[39m\"\u001b[39m\u001b[39mF1 string\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\n\u001b[1;32m    110\u001b[0m     \u001b[39mlambda\u001b[39;00m x: \u001b[39mfloat\u001b[39m(x\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m])\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    113\u001b[0m selection \u001b[39m=\u001b[39m alt\u001b[39m.\u001b[39mselection_point(\n\u001b[1;32m    114\u001b[0m     fields\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mLabel\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    115\u001b[0m     bind\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlegend\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m     value\u001b[39m=\u001b[39m[{\u001b[39m\"\u001b[39m\u001b[39mLabel\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mAverage\u001b[39m\u001b[39m\"\u001b[39m}],\n\u001b[1;32m    117\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "create_dansk_viz(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biases\n",
    "\n",
    "To examine the biases in Danish models we use augmentation to replace names in the Danish dataset DaNE {cite}`hvingelby2020dane`, this approach\n",
    "is similar to that introduced in the initial DaCy paper {cite}`enevoldsen2021dacy`.\n",
    "\n",
    "Here is a short example of how the augmentation might look like:\n",
    "\n",
    "\n",
    "````{admonition} Example\n",
    "\n",
    "```{admonition} Original\n",
    ":class: note\n",
    "\n",
    "\n",
    "Peter Schmeichel mener også, at det danske landshold anno 2021 tilhører verdenstoppen og kan vinde den kommende kamp mod England.\n",
    "```\n",
    "\n",
    "```{admonition} Female name augmentation\n",
    ":class: important\n",
    "\n",
    "Anne Østergaard mener også, at det danske landshold anno 2021 tilhører verdenstoppen og kan vinde den kommende kamp mod England.\n",
    "```\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from performance_testing_utils.ner_bias_utils import apply_models, MDL_GETTER_DICT, apply_models, create_table, get_augmenters\n",
    "from dacy.datasets import dane\n",
    "\n",
    "force = False\n",
    "augmenters = get_augmenters()\n",
    "save_folder = Path(\"performance_tables/ner\")\n",
    "save_folder.mkdir(exist_ok=True, parents=True)\n",
    "dataset = dane(splits = \"test\")\n",
    "\n",
    "tables = []\n",
    "for model_name, getter in MDL_GETTER_DICT.items():\n",
    "    print(model_name)\n",
    "    model_name_ = model_name.replace(\"/\", \"_\")\n",
    "    save_path = save_folder / f\"{model_name_}_bias.csv\"\n",
    "    if not save_path.exists() or force:\n",
    "        nlp = getter()\n",
    "        result_df = apply_models([(model_name, nlp)], dataset, augmenters, n_rep=20)  # type: ignore\n",
    "        result_df.to_csv(save_path, index=False)\n",
    "    else:\n",
    "        print(\"- Already exists, loading in dataframe\")\n",
    "        result_df = pd.read_csv(save_path)\n",
    "    tables.append(result_df)\n",
    "\n",
    "df = pd.concat(tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table(df, augmenters=augmenters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization\n",
    "To examine model generalization, we utilize the [DANSK](https://huggingface.co/datasets/chcaa/DANSK) dataset. This dataset is annotated across many different domains including fiction, web content, social media, wikis, news, legal and conversational data. The original dataset includes annotations corresponding to the ontonotes standard (see [getting started](https://centre-for-humanities-computing.github.io/DaCy/tutorials/basic.html#fine-grained-ner) for the full list). To test the generalization we here convert the annotations to the CoNLL-2003 format using the labels `Person`, `Location`, `Organization`. As CoNLL-2003, `Location` includes cities, roads, mountains, abstract places, specific buildings, and meeting points. Thus the `GPE` (geo-political entity) were converted to `Location`. The `MISC` category in CoNLL-2003 is a diverse category meant to denote all names not in other categories (encapsulating both e.g. events and adjectives such as ”2004 World Cup” and ”Italian”), and is therefore not included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from performance_testing_utils.generalization_utils import dansk, convert_to_conll_2003, MDL_GETTER_DICT, evaluate_generalization, create_generation_viz\n",
    "\n",
    "train, dev, test = dansk()\n",
    "convert_to_conll_2003(train)\n",
    "convert_to_conll_2003(dev)\n",
    "convert_to_conll_2003(test)\n",
    "\n",
    "dataset = train + dev + test\n",
    "\n",
    "assert set([e.label_ for doc in dataset for e in doc.ents]) == set([\"PER\", \"LOC\", \"ORG\"])\n",
    "\n",
    "save_folder = Path(\"performance_tables/ner\")\n",
    "save_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "tables = []\n",
    "# create domains datasets\n",
    "domains = {}\n",
    "for doc in dataset:\n",
    "    domain = doc._.meta[\"dagw_domain\"]\n",
    "    if domain not in domains:\n",
    "        domains[domain] = []\n",
    "    domains[domain].append(doc)\n",
    "\n",
    "for mdl, getter in MDL_GETTER_DICT.items():\n",
    "    mdl_name = mdl.replace(\"/\", \"_\")\n",
    "    save_path = save_folder / f\"{mdl_name}_generalization.csv\"\n",
    "    if not save_path.exists():\n",
    "        nlp = getter()\n",
    "        result_df = evaluate_generalization(mdl_name =mdl, mdl=nlp, domains_dataset_dict=domains)\n",
    "        result_df.to_csv(save_path, index=False)\n",
    "    else:\n",
    "        print(f\"- {mdl} already exists, loading in dataframe\")\n",
    "    result_df = pd.read_csv(save_path) # always load in dataframe to ensure the same representation\n",
    "    tables.append(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(tables)\n",
    "chart = create_generation_viz(df)\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(tables)\n",
    "df = df[df[\"Domain\"] != \"danavis\"]\n",
    "df = df[df[\"Domain\"] != \"dannet\"]\n",
    "df = df[df[\"Domain\"].notnull()]\n",
    "\n",
    "# convert CI to numeric from string\n",
    "df[\"Average F1 CI\"] = df[\"Average F1 CI\"].apply(lambda x: x[1:-1].split(\" \"))\n",
    "df[\"Average F1 CI Lower\"] = df[\"Average F1 CI\"].apply(lambda x: x[0])\n",
    "df[\"Average F1 CI Upper\"] = df[\"Average F1 CI\"].apply(lambda x: x[1])\n",
    "df[\"Average F1 CI Lower\"] = pd.to_numeric(df[\"Average F1 CI Lower\"])\n",
    "df[\"Average F1 CI Upper\"] = pd.to_numeric(df[\"Average F1 CI Upper\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "selection = alt.selection_point(\n",
    "    fields=[\"Domain\"],\n",
    "    bind=\"legend\",\n",
    "    value=[{\"Domain\": \"All\"}],\n",
    ")\n",
    "bind_checkbox = alt.binding_checkbox(\n",
    "    name=\"Scale point size by number of documents: \",\n",
    ")\n",
    "param_checkbox = alt.param(bind=bind_checkbox)\n",
    "\n",
    "base = (\n",
    "    alt.Chart(df)\n",
    "    .mark_point(filled=True)\n",
    "    .encode(\n",
    "        x=alt.X(\"Average F1\", title=\"F1\"),\n",
    "        y=alt.Y(\"Model\", sort=list(MDL_GETTER_DICT.keys())),\n",
    "        color=\"Domain\",\n",
    "        size=alt.condition(param_checkbox, \"Number of docs\", alt.value(100), legend=None),\n",
    "        tooltip=[\n",
    "            \"Model\",\n",
    "            \"Domain\",\n",
    "            \"Average F1\",\n",
    "            \"Person F1\",\n",
    "            \"Location F1\",\n",
    "            \"Organization F1\",\n",
    "        ],\n",
    "        opacity=alt.condition(selection, alt.value(1), alt.value(0.0)),\n",
    "    )\n",
    ")\n",
    "error_bars = (\n",
    "    alt.Chart(df)\n",
    "    .mark_errorbar(ticks=False)\n",
    "    .encode(\n",
    "        # x='Average F1 CI Lower',\n",
    "        x=alt.X(\"Average F1 CI Lower\", title=\"F1\"),\n",
    "        x2=\"Average F1 CI Upper\",\n",
    "        # y=\"Model\",\n",
    "        y=alt.Y(\"Model\", sort=list(MDL_GETTER_DICT.keys())),\n",
    "        color=\"Domain\",\n",
    "        opacity=alt.condition(selection, alt.value(1), alt.value(0.0)),\n",
    "    )\n",
    ")\n",
    "\n",
    "chart =  base + error_bars\n",
    "\n",
    "chart.add_params(selection, param_checkbox).properties(width=800, height=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_sm = spacy.load(\"da_core_news_sm\")\n",
    "sp_lg = spacy.load(\"da_core_news_lg\")\n",
    "\n",
    "mdls = [\n",
    "    (\"spaCy (da_core_news_sm)\", sp_sm),\n",
    "    (\"spaCy (da_core_news_lg)\", sp_lg),\n",
    "]\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "scorer = Scorer()\n",
    "\n",
    "def no_misc_getter(doc, attr):\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ != \"MISC\":\n",
    "            yield ent\n",
    "\n",
    "def bootstrap(examples, n_rep=100):\n",
    "    scores = []\n",
    "    for i in range(n_rep):\n",
    "        sample = random.choices(examples, k=len(examples))\n",
    "        score = scorer.score_spans(sample, getter=no_misc_getter, attr=\"ents\")\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def compute_mean_and_ci(scores):\n",
    "\n",
    "    ent_f = [score[\"ents_f\"] for score in scores]\n",
    "    per_f = [score[\"ents_per_type\"].get(\"PER\", {\"f\": None})[\"f\"] for score in scores]\n",
    "    loc_f = [score[\"ents_per_type\"].get(\"LOC\", {\"f\": None})[\"f\"] for score in scores]\n",
    "    org_f = [score[\"ents_per_type\"].get(\"ORG\", {\"f\": None})[\"f\"] for score in scores]\n",
    "\n",
    "    nam = [\"Average F1\", \"Person F1\", \"Location F1\", \"Organization F1\"]\n",
    "\n",
    "    d = {}\n",
    "    for n, f in zip(nam, [ent_f, per_f, loc_f, org_f]):\n",
    "        f = [x for x in f if x is not None]\n",
    "        if len(f) == 0:\n",
    "            d[n] = {\n",
    "                \"mean\": None,\n",
    "                \"ci\": None\n",
    "            }\n",
    "            continue\n",
    "        d[n] = {\n",
    "            \"mean\": np.mean(f),\n",
    "            \"ci\": np.percentile(f, [2.5, 97.5])\n",
    "        }\n",
    "    return d\n",
    "\n",
    "\n",
    "all_examples = {}\n",
    "rows= []\n",
    "for mdl_name, mdl in mdls:\n",
    "    all_examples[mdl_name] = []\n",
    "    for domain in domains:\n",
    "        docs = domains[domain]\n",
    "        model_pred = mdl.pipe([doc.text for doc in docs])\n",
    "        examples = [Example(predicted=x, reference=y) for x, y in zip(model_pred, docs)]\n",
    "        all_examples[mdl_name].extend(examples)\n",
    "\n",
    "        bs_score = bootstrap(examples)\n",
    "        score = compute_mean_and_ci(bs_score)\n",
    "\n",
    "\n",
    "        row = {\n",
    "            \"Model\": mdl_name,\n",
    "            \"Domain\": domain,\n",
    "            \"Average F1\": score[\"Average F1\"][\"mean\"],\n",
    "            \"Person F1\": score[\"Person F1\"][\"mean\"],\n",
    "            \"Location F1\": score[\"Location F1\"][\"mean\"],\n",
    "            \"Organization F1\": score[\"Organization F1\"][\"mean\"],\n",
    "            \"Average F1 CI\": score[\"Average F1\"][\"ci\"],\n",
    "\n",
    "            \"Number of docs\": len(docs),\n",
    "            \n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "# across domains\n",
    "for mdl in all_examples:\n",
    "    examples = all_examples[mdl]\n",
    "    bs_score = bootstrap(examples)\n",
    "    score = compute_mean_and_ci(bs_score)\n",
    "\n",
    "    row = {\n",
    "        \"Model\": mdl,\n",
    "        \"Domain\": \"All\",\n",
    "        \"Average F1\": score[\"Average F1\"][\"mean\"],\n",
    "        \"Person F1\": score[\"Person F1\"][\"mean\"],\n",
    "        \"Location F1\": score[\"Location F1\"][\"mean\"],\n",
    "        \"Organization F1\": score[\"Organization F1\"][\"mean\"],\n",
    "        \"Average F1 CI\": score[\"Average F1\"][\"ci\"],\n",
    "        \"Number of docs\": len(examples),\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "# write to file\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"ner_performance.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# filter out domains\n",
    "df = df[df[\"Domain\"] != \"danavis\"]\n",
    "df = df[df[\"Domain\"] != \"dannet\"]\n",
    "df = df[df[\"Domain\"].notnull()]\n",
    "\n",
    "df['Average F1 CI Lower'] = df['Average F1 CI'].apply(lambda x: x[0])\n",
    "df['Average F1 CI Upper'] = df['Average F1 CI'].apply(lambda x: x[1])\n",
    "df['Average F1 CI Lower'] = pd.to_numeric(df['Average F1 CI Lower'])\n",
    "df['Average F1 CI Upper'] = pd.to_numeric(df['Average F1 CI Upper'])\n",
    "\n",
    "\n",
    "\n",
    "selection = alt.selection_point(fields=['Domain'], bind='legend', value=[{'Domain': 'All'}]) # does not work\n",
    "\n",
    "bind_checkbox = alt.binding_checkbox(name='Scale point size by number of documents: ')\n",
    "param_checkbox = alt.param(bind=bind_checkbox)\n",
    "\n",
    "base = alt.Chart(df).mark_point(filled=True).encode(\n",
    "    # x='Average F1',\n",
    "    x=alt.X('Average F1', title=\"F1\"),\n",
    "    y='Model',\n",
    "    color='Domain',\n",
    "    size=alt.condition(\n",
    "        param_checkbox,\n",
    "        'Number of docs',\n",
    "        alt.value(100)\n",
    "    ),\n",
    "    tooltip=[\"Model\", \"Domain\", \"Average F1\", \"Person F1\", \"Location F1\", \"Organization F1\"],\n",
    "    opacity=alt.condition(selection, alt.value(1), alt.value(0.0))\n",
    ")\n",
    "error_bars = alt.Chart(df).mark_errorbar(ticks=False).encode(\n",
    "    # x='Average F1 CI Lower',\n",
    "    x = alt.X('Average F1 CI Lower', title=\"F1\"),\n",
    "    x2='Average F1 CI Upper',\n",
    "    y='Model',\n",
    "    color='Domain',\n",
    "    opacity=alt.condition(selection, alt.value(1), alt.value(0.0))\n",
    ")\n",
    "\n",
    "chart = error_bars + base\n",
    "\n",
    "chart.add_params(selection, param_checkbox).properties(\n",
    "    width=800,\n",
    "    height=400\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The F1 in the figure denotes the mean bootstrapped F1 score with a 95% confidence interval. The F1 score is calculated on all of the DANSK dataset.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper [DaCy: A Unified Framework for Danish NLP](https://github.com/centre-for-humanities-computing/DaCy/blob/main/papers/DaCy-A-Unified-Framework-for-Danish-NLP/readme.md) we conducted a series on augmentation on the DaNE test set to estimate the robustness and biases of DaCy and other Danish language processing pipelines. This page represents only parts of the paper. We recommend reading the paper for a more thorough and nuanced overview.\n",
    "\n",
    "Let's start by examining a couple of the augmentations, namely changing out names or introducing plausible keystroke errors.\n",
    "\n",
    "````{admonition} Example\n",
    "\n",
    "```{note} Original\n",
    "\n",
    "Peter Schmeichel mener også, at det danske landshold anno 2021 tilhører verdenstoppen og kan vinde den kommende kamp mod England.\n",
    "```\n",
    "\n",
    "```{important} Female name augmentation\n",
    "\n",
    "Anne Østergaard mener også, at det danske landshold anno 2021 tilhører verdenstoppen og kan vinde den kommende kamp mod England.\n",
    "```\n",
    "````\n",
    "\n",
    "The underlying assumption of making these augmentations is that the tags of the tokens do not change with augmentation. In our case, this includes that \"Anna Østergaard\" is still a person and that \"vonde\" can still be considered a verb based on its context.\n",
    "\n",
    "Based on this, we can assume that if a model performs worse on a certain set of names or with minor spelling variations or errors, we can conclude that the model is vulnerable to such input. For instance, if the model has a hard time when replacing æ, ø, and å with ae, oe, and aa, it might not be ideal to apply to historic texts.\n",
    "\n",
    "As seen in the example above, while text with 5% keystroke is still readable. However, 15% keystroke errors tests the limit of what humans and models can reasonably be expected to comprehend.\n",
    "\n",
    "```{important}\n",
    "**15% keytype errors**\n",
    "\n",
    "Peter Schmeichel mejer ogsp, at ddt danske landshoof anbo 202q tilhårer gerfenatop0en of lan vinde sen kpmkendw lamp mod England.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The following tables show a detailed breakdown of performance for named entity recognition, part-of-speech tagging, and dependency parsing. These show some general trends, some of which include:\n",
    "\n",
    "- Spelling variations and abbreviated first names consistently reduce performance of all models on all tasks.\n",
    "- Even simple replacements of æ, ø, and å with ae, oe, and aa lead to notable performance degradation.\n",
    "- In general, larger models handle augmentations better than small models with DaCy large performing the best.\n",
    "- The BiLSTM-based models (Stanza and Flair) perform competitively under augmentations and are only consistently outperformed by DaCy large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from performance_testing_utils.ner_bias_utils import apply_models, MDL_GETTER_DICT, apply_models, create_table\n",
    "from performance_testing_utils.ner_robustness_utils import get_augmenters\n",
    "\n",
    "force = False\n",
    "augmenters = get_augmenters()\n",
    "save_folder = Path(\"performance_tables/ner\")\n",
    "save_folder.mkdir(exist_ok=True, parents=True)\n",
    "dataset = dane(splits = \"test\")\n",
    "\n",
    "tables = []\n",
    "for model_name, getter in MDL_GETTER_DICT.items():\n",
    "    print(model_name)\n",
    "    model_name_ = model_name.replace(\"/\", \"_\")\n",
    "    save_path = save_folder / f\"{model_name_}_bias.csv\"\n",
    "    if not save_path.exists() or force:\n",
    "        nlp = getter()\n",
    "        result_df = apply_models([(model_name, nlp)], dataset, augmenters, n_rep=20)  # type: ignore\n",
    "        result_df.to_csv(save_path, index=False)\n",
    "    else:\n",
    "        print(\"- Already exists, loading in dataframe\")\n",
    "        result_df = pd.read_csv(save_path)\n",
    "    tables.append(result_df)\n",
    "\n",
    "df = pd.concat(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table(df, augmenters=augmenters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how does these augmentations look? The following shows an example of the augmentation using a sample for DaNE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dacy.datasets import dane\n",
    "import spacy\n",
    "import augmenty\n",
    "\n",
    "nlp = spacy.blank(\"da\")\n",
    "test_corpus = dane(splits = \"test\")\n",
    "\n",
    "example = next(test_corpus(nlp)) # extract first example\n",
    "doc = example.reference  # extract the reference/gold standard document\n",
    "\n",
    "print(doc)\n",
    "\n",
    "for augmentation_name, augmenter in augmenters:\n",
    "    print(augmentation_name)\n",
    "    augmented_docs = augmenty.docs([doc], augmenter)\n",
    "    for augmented_doc in augmented_docs:\n",
    "        print(\"\\t-\", augmented_doc)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "```{bibliography}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
