{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/KennethEnevoldsen/DaCy/blob/master/docs/performance.ner.ipynb)\n",
    "\n",
    "\n",
    "This page examines the performance of competing models for Danish named entity recognition over multiple datasets. Performance is not limited to \n",
    "accuracy, but also includes domain generalization, biases and robustness. This page is also a notebook, which open and replicate the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State-of-the-Art comparison\n",
    "To our knowledge there exists three datasets for Danish named entity recognition;\n",
    "\n",
    "1) DaNE {cite}`hvingelby2020dane`, which uses the simple annotation scheme of CoNLL 2003 {cite}`missing` with the entities; *person*, *location*, *organization*, and *miscellaneus*.\n",
    "2) DANSK {cite}`missing`, which uses the extensive annotation scheme similar to that of OntoNotes 5.0 {cite}`missing` including more that 16 entity types.\n",
    "3) and DAN+ {cite}`missing`, which also uses the annotation scheme of CoNLL 2003, but allows for nested entities for instance *Aarhus Universitet*, where *Aarhus* is a location and *Aarhus Universitet* is an organization.\n",
    "\n",
    "In this comparison we will be examing performance on DaNE and DANSK, but as no known models have been trained on Danish nested entities, we will not be comparing performance on DAN+.\n",
    "\n",
    "\n",
    "```{admonition} Measuring Performance\n",
    "Typically when measuring performance on these benchmark it is normal to feed the model the gold standard tokens. While this allows for easier comparisons of modules and architectures, it inflates the performance metrics. Further, it does not proberly reflect what you are really interested in:\n",
    "*the performance you can expect when you apply the model to data of a similar type*. Therefore we estimate the model is given no prior knowledge of the data, and only the raw text is fed to the model. Thus the performance metrics might be slightly different compared to e.g. DaNLP.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DaNE: Simple Named Entity Recognition\n",
    "As already stated DaNE uses an extraction from the CoNLL 2003 dataset, which is as follows {cite}`hvingelby2020dane`:\n",
    "\n",
    "\n",
    "| Entity | Description |\n",
    "|--------------|-------------|\n",
    "| LOC          | includes locations like cities, roads and mountains, as well as both public and commercial places like specific buildings or meeting points, but also abstract places. |\n",
    "| PERSON | consists of names of people, fictional characters, and animals. The names includes aliases. |\n",
    "| ORG | can be summarized as all sorts of organizations and collections of people, ranging from companies, brands, political movements, governmental bodies and clubs. |\n",
    "| MISC | is a broad category of e.g. events, languages, titles and religions, but this tag also includes words derived from one of the four tags as well as words for which one part is from one of the three other tags. |\n",
    "\n",
    "Here is an example from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">To kendte \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    russiske\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " historikere \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Andronik Mirganjan\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " og \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Igor Klamkin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " tror ikke, at \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Rusland\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " kan udvikles uden en &quot;jernnæve&quot;.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n",
    "\n",
    "text = \"\"\"To kendte russiske historikere Andronik Mirganjan og Igor Klamkin tror ikke, at Rusland kan udvikles uden en \"jernnæve\".\"\"\"\n",
    "nlp = spacy.blank(\"da\")\n",
    "doc = nlp(text)\n",
    "doc.ents = [  # type: ignore\n",
    "    Span(doc, 2, 3, label=\"MISC\"),\n",
    "    Span(doc, 4, 6, label=\"PERSON\"),\n",
    "    Span(doc, 7, 9, label=\"PERSON\"),\n",
    "    Span(doc, 13, 14, label=\"LOC\"),\n",
    "]\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the performance of Danish language processing pipelines scored on the DaNE test set. The best scores in each category are highlighted with bold and the second best is underlined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dane (test): Loading prediction for saattrupdan/nbailab-base-ner-scandi\n",
      "dane (test): Loading prediction for da_dacy_large_trf-0.2.0\n",
      "dane (test): Loading prediction for da_dacy_medium_trf-0.2.0\n",
      "dane (test): Loading prediction for da_dacy_small_trf-0.2.0\n",
      "dane (test): Running da_dacy_large_ner_fine_grained-0.1.0\n",
      "dane (test): Running da_dacy_medium_ner_fine_grained-0.1.0\n",
      "dane (test): Running da_dacy_small_ner_fine_grained-0.1.0\n",
      "dane (test): Running alexandrainst/da-ner-base\n",
      "dane (test): Running da_core_news_trf-3.5.0\n",
      "dane (test): Running da_core_news_lg-3.5.0\n",
      "dane (test): Running da_core_news_md-3.5.0\n",
      "dane (test): Running da_core_news_sm-3.5.0\n"
     ]
    }
   ],
   "source": [
    "## Apply each of the model to DaNE\n",
    "from pathlib import Path\n",
    "from re import L\n",
    "from typing import Callable, List\n",
    "\n",
    "from numpy import save\n",
    "\n",
    "from evaluation.models import MODELS\n",
    "from evaluation.datasets import datasets\n",
    "from spacy.training import Example\n",
    "from spacy.language import Language\n",
    "import spacy\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "import json\n",
    "\n",
    "\n",
    "def doc_to_json(doc: Doc):\n",
    "    json_obj = doc.to_json()\n",
    "    if hasattr(doc._, \"meta\"):\n",
    "        json_obj[\"meta\"] = doc._.meta\n",
    "    return json_obj\n",
    "\n",
    "\n",
    "def doc_from_json(json_obj: dict, nlp: Language):\n",
    "    doc = Doc(nlp.vocab).from_json(json_obj)\n",
    "    if \"meta\" in json_obj:\n",
    "        if not Doc.has_extension(\"meta\"):\n",
    "            Doc.set_extension(\"meta\", default={}, force=True)\n",
    "        doc._.meta = json_obj[\"meta\"]\n",
    "    return doc\n",
    "\n",
    "\n",
    "def predictions_to_disk(\n",
    "    save_path: Path, examples: List[Example], mdl_name: str, time_in_seconds: float\n",
    "):\n",
    "    save_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    meta = {\n",
    "        \"mdl_name\": mdl_name,\n",
    "        \"time_in_seconds\": time_in_seconds,\n",
    "        \"Hardware\": \"Apple M1 Pro 16Gb running macOS 13.3.1\",\n",
    "    }\n",
    "\n",
    "    # write to json\n",
    "    meta[\"predicted\"] = [doc_to_json(d.predicted) for d in examples]\n",
    "    meta[\"reference\"] = [doc_to_json(d.reference) for d in examples]\n",
    "\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    meta[\"examples\"] = examples\n",
    "    return meta\n",
    "\n",
    "\n",
    "def predictions_from_disk(path: Path) -> dict:\n",
    "    nlp = spacy.blank(\"da\")\n",
    "    with open(path) as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    reference = [doc_from_json(d, nlp) for d in meta[\"reference\"]]\n",
    "    predicted = [doc_from_json(d, nlp) for d in meta[\"predicted\"]]\n",
    "\n",
    "    examples = []\n",
    "    for ref, pred in zip(reference, predicted):\n",
    "        example = Example(reference=ref, predicted=pred)\n",
    "        examples.append(example)\n",
    "\n",
    "    meta[\"examples\"] = examples\n",
    "\n",
    "    return meta\n",
    "\n",
    "\n",
    "def apply_models(\n",
    "    mdl_name,\n",
    "    mdl_getter: Callable[[], Language],\n",
    "    dataset: str,\n",
    "    splits: list[str] = [\"test\"],\n",
    "    cache: bool = True,\n",
    "):\n",
    "    from time import time\n",
    "\n",
    "    docs_path = Path(\".\")\n",
    "    _mdl_name = mdl_name.replace(\"/\", \"_\")\n",
    "    save_folder = docs_path / \"evaluation\" / \"data\" / f\"{_mdl_name}\"\n",
    "\n",
    "    results = {}\n",
    "    for split in splits:\n",
    "        save_path = save_folder / f\"{dataset}_{split}.json\"\n",
    "        if not save_path.exists() and cache:\n",
    "            print(f\"{dataset} ({split}): Running {mdl_name}\")\n",
    "            dataset_getter = datasets.get(dataset)\n",
    "            examples = dataset_getter()[split]\n",
    "            nlp = mdl_getter()\n",
    "\n",
    "            start = time()\n",
    "            docs = nlp.pipe(example.reference.text for example in examples)\n",
    "            for doc, example in zip(docs, examples):\n",
    "                example.predicted = doc\n",
    "            end = time()\n",
    "            time_in_seconds = end - start\n",
    "            results = predictions_to_disk(\n",
    "                save_path, examples, mdl_name, time_in_seconds\n",
    "            )\n",
    "        else:\n",
    "            print(f\"{dataset} ({split}): Loading prediction for {mdl_name}\")\n",
    "\n",
    "        results[split] = predictions_from_disk(save_path)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "dane = {}\n",
    "for mdl_name, model_getter in MODELS.items():\n",
    "    mdl_results = apply_models(mdl_name, model_getter, dataset=\"dane\", splits=[\"test\"])\n",
    "    dane[mdl_name] = mdl_results[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional\n",
    "import random\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def bootstrap(\n",
    "    examples: List[Example], n_rep: int = 100, getter: Optional[Callable] = None\n",
    "):\n",
    "    scorer = Scorer()\n",
    "    scores = []\n",
    "    for _i in range(n_rep):\n",
    "        sample = random.choices(examples, k=len(examples))\n",
    "        if getter is None:\n",
    "            score = scorer.score_spans(sample, attr=\"ents\")\n",
    "        else:\n",
    "            score = scorer.score_spans(sample, getter=getter, attr=\"ents\")\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def compute_mean_and_ci(scores):\n",
    "    ent_f = [score[\"ents_f\"] for score in scores]\n",
    "    # filter out None\n",
    "    ent_f = [x for x in ent_f if x is not None]\n",
    "    if ent_f:\n",
    "        result_dict = {\n",
    "            \"Average\": {\"mean\": np.mean(ent_f), \"ci\": np.percentile(ent_f, [2.5, 97.5])}\n",
    "        }\n",
    "    else:\n",
    "        result_dict = {\"Average\": {\"mean\": None, \"ci\": None}}\n",
    "\n",
    "    score_mapping = {\n",
    "        \"PER\": \"Person\",\n",
    "        \"LOC\": \"Location\",\n",
    "        \"LOCATION\": \"Location\",\n",
    "        \"ORG\": \"Organization\",\n",
    "        \"LANGUAGE\": \"Language\",\n",
    "        \"PRODUCT\": \"Product\",\n",
    "        \"LAW\": \"Law\",\n",
    "        \"ORGANIZATION\": \"Organization\",\n",
    "        \"WORK OF ART\": \"Work of Art\",\n",
    "        \"PERSON\": \"Person\",\n",
    "        \"FACILITY\": \"Facility\",\n",
    "        \"GPE\": \"GPE\",\n",
    "        \"EVENT\": \"Event\",\n",
    "        \"CARDINAL\": \"Cardinal\",\n",
    "        \"DATE\": \"Date\",\n",
    "        \"MONEY\": \"Money\",\n",
    "        \"NORP\": \"NORP\",\n",
    "        \"ORDINAL\": \"Ordinal\",\n",
    "        \"PERCENT\": \"Percent\",\n",
    "        \"QUANTITY\": \"Quantity\",\n",
    "        \"TIME\": \"Time\",\n",
    "        \"MISC\": \"Misc.\",\n",
    "    }\n",
    "\n",
    "    labels = set([label for score in scores for label in score[\"ents_per_type\"]])\n",
    "\n",
    "    for label in labels:\n",
    "        label_f = [\n",
    "            score[\"ents_per_type\"].get(label, {\"f\": None})[\"f\"] for score in scores\n",
    "        ]\n",
    "        label_f = [x for x in label_f if x is not None]\n",
    "        label = score_mapping.get(label, label)\n",
    "        if len(label_f) == 0:\n",
    "            result_dict[label] = {\"mean\": None, \"ci\": None}\n",
    "            continue\n",
    "        result_dict[label] = {\n",
    "            \"mean\": np.mean(label_f),\n",
    "            \"ci\": np.percentile(label_f, [2.5, 97.5]),\n",
    "        }\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def create_dataframe(\n",
    "    examples: List[Example], mdl_name: str, decimals: int = 1, n_rep: int = 100\n",
    "):\n",
    "    score = bootstrap(examples, getter=None, n_rep=n_rep)\n",
    "    score = compute_mean_and_ci(score)\n",
    "\n",
    "    row = {\n",
    "        \"Models\": mdl_name,\n",
    "    }\n",
    "\n",
    "    def score_to_string(score: Dict[str, Any], decimals: int = 1) -> str:\n",
    "        if score[\"mean\"] == 0:\n",
    "            return \" \"\n",
    "        return f\"{100*score['mean']:.{decimals}f} ({100*score['ci'][0]:.{decimals}f}, {100*score['ci'][1]:.{decimals}f})\"\n",
    "\n",
    "    for key, value in score.items():\n",
    "        row[key] = score_to_string(value, decimals=decimals)\n",
    "    return pd.DataFrame([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(s: pd.Series) -> list:\n",
    "    \"\"\"Highlight the maximum in a Series with bold text.\"\"\"\n",
    "    # convert to str for comparison\n",
    "    s = s.astype(str)\n",
    "    is_max = s == s.max()\n",
    "    return [\"font-weight: bold\" if v else \"\" for v in is_max]\n",
    "\n",
    "\n",
    "def underline_second_max(s: pd.Series) -> list:\n",
    "    \"\"\"Underline the second maximum in a Series.\"\"\"\n",
    "    is_second_max = s == s.sort_values(ascending=False).iloc[1]\n",
    "    return [\"text-decoration: underline\" if v else \"\" for v in is_second_max]\n",
    "\n",
    "\n",
    "def create_table(\n",
    "    df: pd.DataFrame,\n",
    "    caption=\"F1 score with 95% confidence interval calculated using bootstrapping with 100 samples.\",\n",
    "):\n",
    "    # replace index with range\n",
    "    df.index = range(len(df))  # type: ignore\n",
    "\n",
    "    col_names = [(\"\", \"Models\")] + [(\"F1\", col) for col in df.columns[1:]]\n",
    "    super_header = pd.MultiIndex.from_tuples(col_names)\n",
    "    df.columns = super_header\n",
    "\n",
    "    s = df.style.apply(highlight_max, axis=0, subset=df.columns[1:])\n",
    "    s = s.apply(underline_second_max, axis=0, subset=df.columns[1:])\n",
    "\n",
    "    # Add a caption\n",
    "    s = s.set_caption(caption)\n",
    "\n",
    "    # Center the header and left align the model names\n",
    "    s = s.set_properties(subset=df.columns[1:], **{\"text-align\": \"right\"})\n",
    "\n",
    "    super_header_style = [\n",
    "        {\"selector\": \".level0\", \"props\": [(\"text-align\", \"center\")]},\n",
    "        {\"selector\": \".col_heading\", \"props\": [(\"text-align\", \"center\")]},\n",
    "    ]\n",
    "    # Apply the CSS style to the styler\n",
    "    s = s.set_table_styles(super_header_style)  # type: ignore\n",
    "    s = s.set_properties(subset=[(\"\", \"Models\")], **{\"text-align\": \"left\"})\n",
    "    # remove the index\n",
    "    s = s.hide(axis=\"index\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1f845 .level0 {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_1f845 .col_heading {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_1f845_row0_col0, #T_1f845_row1_col0, #T_1f845_row2_col0, #T_1f845_row3_col0, #T_1f845_row4_col0, #T_1f845_row5_col0, #T_1f845_row6_col0, #T_1f845_row7_col0, #T_1f845_row8_col0 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_1f845_row0_col1, #T_1f845_row0_col3, #T_1f845_row0_col4, #T_1f845_row1_col2, #T_1f845_row1_col5 {\n",
       "  font-weight: bold;\n",
       "  text-align: right;\n",
       "}\n",
       "#T_1f845_row0_col2, #T_1f845_row1_col1, #T_1f845_row1_col3, #T_1f845_row1_col4, #T_1f845_row2_col5 {\n",
       "  text-decoration: underline;\n",
       "  text-align: right;\n",
       "}\n",
       "#T_1f845_row0_col5, #T_1f845_row2_col1, #T_1f845_row2_col2, #T_1f845_row2_col3, #T_1f845_row2_col4, #T_1f845_row3_col1, #T_1f845_row3_col2, #T_1f845_row3_col3, #T_1f845_row3_col4, #T_1f845_row3_col5, #T_1f845_row4_col1, #T_1f845_row4_col2, #T_1f845_row4_col3, #T_1f845_row4_col4, #T_1f845_row4_col5, #T_1f845_row5_col1, #T_1f845_row5_col2, #T_1f845_row5_col3, #T_1f845_row5_col4, #T_1f845_row5_col5, #T_1f845_row6_col1, #T_1f845_row6_col2, #T_1f845_row6_col3, #T_1f845_row6_col4, #T_1f845_row6_col5, #T_1f845_row7_col1, #T_1f845_row7_col2, #T_1f845_row7_col3, #T_1f845_row7_col4, #T_1f845_row7_col5, #T_1f845_row8_col1, #T_1f845_row8_col2, #T_1f845_row8_col3, #T_1f845_row8_col4, #T_1f845_row8_col5 {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1f845\">\n",
       "  <caption>F1 score with 95% confidence interval calculated using bootstrapping with 500 samples.</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_1f845_level0_col0\" class=\"col_heading level0 col0\" ></th>\n",
       "      <th id=\"T_1f845_level0_col1\" class=\"col_heading level0 col1\" colspan=\"5\">F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f845_level1_col0\" class=\"col_heading level1 col0\" >Models</th>\n",
       "      <th id=\"T_1f845_level1_col1\" class=\"col_heading level1 col1\" >Average</th>\n",
       "      <th id=\"T_1f845_level1_col2\" class=\"col_heading level1 col2\" >Location</th>\n",
       "      <th id=\"T_1f845_level1_col3\" class=\"col_heading level1 col3\" >Person</th>\n",
       "      <th id=\"T_1f845_level1_col4\" class=\"col_heading level1 col4\" >Organization</th>\n",
       "      <th id=\"T_1f845_level1_col5\" class=\"col_heading level1 col5\" >Misc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_1f845_row0_col0\" class=\"data row0 col0\" >saattrupdan/nbailab-base-ner-scandi</td>\n",
       "      <td id=\"T_1f845_row0_col1\" class=\"data row0 col1\" >86.1 (82.4, 89.4)</td>\n",
       "      <td id=\"T_1f845_row0_col2\" class=\"data row0 col2\" >88.2 (82.2, 92.9)</td>\n",
       "      <td id=\"T_1f845_row0_col3\" class=\"data row0 col3\" >94.9 (91.6, 97.8)</td>\n",
       "      <td id=\"T_1f845_row0_col4\" class=\"data row0 col4\" >80.3 (74.4, 86.0)</td>\n",
       "      <td id=\"T_1f845_row0_col5\" class=\"data row0 col5\" >78.4 (70.2, 85.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f845_row1_col0\" class=\"data row1 col0\" >da_dacy_large_trf-0.2.0</td>\n",
       "      <td id=\"T_1f845_row1_col1\" class=\"data row1 col1\" >85.3 (81.4, 89.0)</td>\n",
       "      <td id=\"T_1f845_row1_col2\" class=\"data row1 col2\" >89.1 (82.9, 94.0)</td>\n",
       "      <td id=\"T_1f845_row1_col3\" class=\"data row1 col3\" >92.7 (89.0, 95.9)</td>\n",
       "      <td id=\"T_1f845_row1_col4\" class=\"data row1 col4\" >79.0 (72.3, 85.1)</td>\n",
       "      <td id=\"T_1f845_row1_col5\" class=\"data row1 col5\" >78.9 (70.8, 86.2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f845_row2_col0\" class=\"data row2 col0\" >da_dacy_medium_trf-0.2.0</td>\n",
       "      <td id=\"T_1f845_row2_col1\" class=\"data row2 col1\" >84.8 (80.6, 88.6)</td>\n",
       "      <td id=\"T_1f845_row2_col2\" class=\"data row2 col2\" >86.6 (80.9, 91.4)</td>\n",
       "      <td id=\"T_1f845_row2_col3\" class=\"data row2 col3\" >92.5 (88.7, 95.4)</td>\n",
       "      <td id=\"T_1f845_row2_col4\" class=\"data row2 col4\" >78.5 (71.4, 84.9)</td>\n",
       "      <td id=\"T_1f845_row2_col5\" class=\"data row2 col5\" >78.9 (70.6, 86.1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f845_row3_col0\" class=\"data row3 col0\" >da_dacy_small_trf-0.2.0</td>\n",
       "      <td id=\"T_1f845_row3_col1\" class=\"data row3 col1\" >82.6 (79.4, 85.7)</td>\n",
       "      <td id=\"T_1f845_row3_col2\" class=\"data row3 col2\" >83.8 (77.9, 89.4)</td>\n",
       "      <td id=\"T_1f845_row3_col3\" class=\"data row3 col3\" >92.0 (88.1, 95.0)</td>\n",
       "      <td id=\"T_1f845_row3_col4\" class=\"data row3 col4\" >75.6 (69.7, 81.2)</td>\n",
       "      <td id=\"T_1f845_row3_col5\" class=\"data row3 col5\" >75.9 (69.2, 82.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f845_row4_col0\" class=\"data row4 col0\" >alexandrainst/da-ner-base</td>\n",
       "      <td id=\"T_1f845_row4_col1\" class=\"data row4 col1\" >70.6 (66.1, 74.9)</td>\n",
       "      <td id=\"T_1f845_row4_col2\" class=\"data row4 col2\" >84.7 (78.3, 89.8)</td>\n",
       "      <td id=\"T_1f845_row4_col3\" class=\"data row4 col3\" >90.2 (86.0, 93.8)</td>\n",
       "      <td id=\"T_1f845_row4_col4\" class=\"data row4 col4\" >64.7 (57.0, 71.4)</td>\n",
       "      <td id=\"T_1f845_row4_col5\" class=\"data row4 col5\" > </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f845_row5_col0\" class=\"data row5 col0\" >da_core_news_trf-3.5.0</td>\n",
       "      <td id=\"T_1f845_row5_col1\" class=\"data row5 col1\" >78.8 (74.9, 82.3)</td>\n",
       "      <td id=\"T_1f845_row5_col2\" class=\"data row5 col2\" >81.9 (74.9, 88.0)</td>\n",
       "      <td id=\"T_1f845_row5_col3\" class=\"data row5 col3\" >91.5 (87.9, 94.4)</td>\n",
       "      <td id=\"T_1f845_row5_col4\" class=\"data row5 col4\" >68.1 (60.7, 74.7)</td>\n",
       "      <td id=\"T_1f845_row5_col5\" class=\"data row5 col5\" >68.8 (60.2, 76.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f845_row6_col0\" class=\"data row6 col0\" >da_core_news_lg-3.5.0</td>\n",
       "      <td id=\"T_1f845_row6_col1\" class=\"data row6 col1\" >74.5 (70.6, 78.8)</td>\n",
       "      <td id=\"T_1f845_row6_col2\" class=\"data row6 col2\" >81.5 (74.4, 87.7)</td>\n",
       "      <td id=\"T_1f845_row6_col3\" class=\"data row6 col3\" >85.3 (80.4, 89.4)</td>\n",
       "      <td id=\"T_1f845_row6_col4\" class=\"data row6 col4\" >62.6 (54.5, 69.7)</td>\n",
       "      <td id=\"T_1f845_row6_col5\" class=\"data row6 col5\" >64.4 (55.5, 72.6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f845_row7_col0\" class=\"data row7 col0\" >da_core_news_md-3.5.0</td>\n",
       "      <td id=\"T_1f845_row7_col1\" class=\"data row7 col1\" >71.0 (67.1, 74.8)</td>\n",
       "      <td id=\"T_1f845_row7_col2\" class=\"data row7 col2\" >76.7 (69.2, 83.6)</td>\n",
       "      <td id=\"T_1f845_row7_col3\" class=\"data row7 col3\" >82.3 (77.2, 86.6)</td>\n",
       "      <td id=\"T_1f845_row7_col4\" class=\"data row7 col4\" >58.2 (50.1, 66.0)</td>\n",
       "      <td id=\"T_1f845_row7_col5\" class=\"data row7 col5\" >61.6 (53.3, 70.1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f845_row8_col0\" class=\"data row8 col0\" >da_core_news_sm-3.5.0</td>\n",
       "      <td id=\"T_1f845_row8_col1\" class=\"data row8 col1\" >64.2 (60.1, 68.4)</td>\n",
       "      <td id=\"T_1f845_row8_col2\" class=\"data row8 col2\" >61.5 (52.4, 70.1)</td>\n",
       "      <td id=\"T_1f845_row8_col3\" class=\"data row8 col3\" >79.7 (74.6, 84.4)</td>\n",
       "      <td id=\"T_1f845_row8_col4\" class=\"data row8 col4\" >49.1 (40.3, 57.6)</td>\n",
       "      <td id=\"T_1f845_row8_col5\" class=\"data row8 col5\" >58.4 (49.2, 66.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2a8f7d840>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = []\n",
    "for mdl in dane:\n",
    "    # skip fine grained NER models for DaNE\n",
    "    if \"fine_grained\" in mdl:\n",
    "        continue\n",
    "    tables.append(create_dataframe(dane[mdl][\"examples\"], mdl, n_rep=500))\n",
    "\n",
    "df = pd.concat(tables)\n",
    "# sort columns\n",
    "df = df[[\"Models\", \"Average\", \"Location\", \"Person\", \"Organization\", \"Misc.\"]]\n",
    "create_table(\n",
    "    df,\n",
    "    \"F1 score with 95% confidence interval calculated using bootstrapping with 500 samples.\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth mentioning that while the `da_dacy_large_trf-0.2.0` and `saattrupdan/nbailab-base-ner-scandi` performs similarly they have their independent strength and weaknesses. The large DaCy model is a multi-task model performing named-entity recognition as only one of its many tasks and thus if you wish to use one of those we would recommend that model. On the other hand the `nbailab-base-ner-scandi` is trained on multiple Scandinavian languages and thus might be ideal if your dataset might contain these languages as well. `saattrupdan/nbailab-base-ner-scandi` is available in DaCy using `nlp.add_pipe(\"dacy/ner\")`.\n",
    "\n",
    "```{admonition} You are missing a model\n",
    ":note:\n",
    "\n",
    "These tables are continually updated and thus we try to limit the number of models to only the most relevant Danish models. Therefore models like Polyglot with strict requirements and consistently worse performance are excluded. If you want to see a specific model, please open an issue on GitHub.\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DANSK: Fine-grained Named Entity Recognition\n",
    "\n",
    "DANSK is annotated from the Danish Gigaword Corpus {cite}`missing` and a wide variety of domains including conversational, legal, news, social media, web content,  wiki's and Books. Dansk follows includes the following labels:\n",
    "\n",
    "\n",
    "|  Entity        |             Description                                         |\n",
    "| -------- | ---------------------------------------------------- |\n",
    "| PERSON   | People, including fictional                          |\n",
    "| NORP     | Nationalities or religious or political groups       |\n",
    "| FACILITY | Building, airports, highways, bridges, etc.          |\n",
    "| ORGANIZATION | Companies, agencies, institutions, etc.              |\n",
    "| GPE      | Countries, cities, states.                           |\n",
    "| LOCATION | Non-GPE locations, mountain ranges, bodies of water  |\n",
    "| PRODUCT  | Vehicles, weapons, foods, etc. (not services)        |\n",
    "| EVENT    | Named hurricanes, battles, wars, sports events, etc. |\n",
    "| WORK OF ART | Titles of books, songs, etc.                         |\n",
    "| LAW      | Named documents made into laws                       |\n",
    "| LANGUAGE | Any named language                                   |\n",
    "\n",
    "As well as annotation for the following concepts:\n",
    "\n",
    "|   Entity       |   Description                                         |\n",
    "| -------- | ------------------------------------------- |\n",
    "| DATE     | Absolute or relative dates or periods       |\n",
    "| TIME     | Times smaller than a day                    |\n",
    "| PERCENT  | Percentage (including \"*\"%)                |\n",
    "| MONEY    | Monetary values, including unit             |\n",
    "| QUANTITY | Measurements, as of weight or distance      |\n",
    "| ORDINAL  | \"first\", \"second\"                           |\n",
    "| CARDINAL | Numerals that do no fall under another type |\n",
    "\n",
    "\n",
    "We have here opted to create an interactive chart over a table as with the number of labels it quickly becomes unruly. The chart is interactive and you can select the label you want to compare the models on. You can also hover over the dots the see the exact values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dansk (train): Running saattrupdan/nbailab-base-ner-scandi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/au561649/.cache/huggingface/datasets/chcaa___parquet/chcaa--DANSK-ec592bb9b8d7fe08/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/Users/au561649/.cache/huggingface/datasets/chcaa___parquet/chcaa--DANSK-ec592bb9b8d7fe08/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/Users/au561649/.cache/huggingface/datasets/chcaa___parquet/chcaa--DANSK-ec592bb9b8d7fe08/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dansk (dev): Running saattrupdan/nbailab-base-ner-scandi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/au561649/.cache/huggingface/datasets/chcaa___parquet/chcaa--DANSK-ec592bb9b8d7fe08/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/Users/au561649/.cache/huggingface/datasets/chcaa___parquet/chcaa--DANSK-ec592bb9b8d7fe08/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/Users/au561649/.cache/huggingface/datasets/chcaa___parquet/chcaa--DANSK-ec592bb9b8d7fe08/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dansk (test): Running saattrupdan/nbailab-base-ner-scandi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/au561649/.cache/huggingface/datasets/chcaa___parquet/chcaa--DANSK-ec592bb9b8d7fe08/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/Users/au561649/.cache/huggingface/datasets/chcaa___parquet/chcaa--DANSK-ec592bb9b8d7fe08/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/Users/au561649/.cache/huggingface/datasets/chcaa___parquet/chcaa--DANSK-ec592bb9b8d7fe08/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dansk (train): Running da_dacy_large_trf-0.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/au561649/.cache/huggingface/datasets/chcaa___parquet/chcaa--DANSK-ec592bb9b8d7fe08/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/Users/au561649/.cache/huggingface/datasets/chcaa___parquet/chcaa--DANSK-ec592bb9b8d7fe08/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/Users/au561649/.cache/huggingface/datasets/chcaa___parquet/chcaa--DANSK-ec592bb9b8d7fe08/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "dansk = {}\n",
    "for mdl_name, model_getter in MODELS.items():\n",
    "    mdl_results = apply_models(\n",
    "        mdl_name, model_getter, dataset=\"dansk\", splits=[\"train\", \"dev\", \"test\"]\n",
    "    )\n",
    "    dansk[mdl_name] = mdl_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = []\n",
    "for mdl in dane:\n",
    "    # only fine-grained models for DANSK\n",
    "    if \"fine_grained\" not in mdl:\n",
    "        continue\n",
    "    tables.append(create_dataframe(dansk[mdl][\"test\"][\"examples\"], mdl, n_rep=100))\n",
    "\n",
    "df = pd.concat(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "\n",
    "def create_dansk_viz(df: pd.DataFrame):\n",
    "    plot_df = df.melt(\n",
    "        id_vars=[\"Models\"],\n",
    "        var_name=\"Label\",\n",
    "        value_name=\"F1 string\",\n",
    "    )\n",
    "\n",
    "    # Convert the score value to a float\n",
    "    plot_df[\"F1\"] = plot_df[\"F1 string\"].apply(\n",
    "        lambda x: float(x.split()[0]) if not isinstance(x, float) else x\n",
    "    )\n",
    "    plot_df[\"CI Lower\"] = plot_df[\"F1 string\"].apply(\n",
    "        lambda x: float(x.split(\"(\")[1].split(\",\")[0])\n",
    "    )\n",
    "    plot_df[\"CI Upper\"] = plot_df[\"F1 string\"].apply(\n",
    "        lambda x: float(x.split(\",\")[1].split(\")\")[0])\n",
    "    )\n",
    "\n",
    "    selection = alt.selection_point(\n",
    "        fields=[\"Label\"],\n",
    "        bind=\"legend\",\n",
    "        value=[{\"Label\": \"Average\"}],\n",
    "    )\n",
    "\n",
    "    base = (\n",
    "        alt.Chart(plot_df)\n",
    "        .mark_point(filled=True, size=100)\n",
    "        .encode(\n",
    "            x=alt.X(\"F1\", title=\"F1\"),\n",
    "            y=\"Models\",\n",
    "            color=\"Label\",\n",
    "            tooltip=[\n",
    "                \"Models\",\n",
    "                \"Label\",\n",
    "                alt.Tooltip(\"F1 string\", title=\"F1\"),\n",
    "            ],\n",
    "            opacity=alt.condition(selection, alt.value(1), alt.value(0.0)),\n",
    "            # only show the tooltip when when the label is selected\n",
    "        )\n",
    "    )\n",
    "    error_bars = (\n",
    "        alt.Chart(plot_df)\n",
    "        .mark_errorbar(ticks=False)\n",
    "        .encode(\n",
    "            x=alt.X(\"CI Lower\", title=\"F1\"),\n",
    "            x2=\"CI Upper\",\n",
    "            y=\"Models\",\n",
    "            color=\"Label\",\n",
    "            opacity=alt.condition(selection, alt.value(1), alt.value(0.0)),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    chart = base + error_bars\n",
    "\n",
    "    return chart.add_params(selection).properties(width=800, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dansk_viz(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df\n",
    "_df = _df.set_index(\"Models\")\n",
    "ent_columns = sorted(\n",
    "    [\n",
    "        \"Event\",\n",
    "        \"Organization\",\n",
    "        \"Language\",\n",
    "        \"Person\",\n",
    "        \"Ordinal\",\n",
    "        \"NORP\",\n",
    "        \"Work of Art\",\n",
    "        \"Facility\",\n",
    "        \"Law\",\n",
    "        \"Location\",\n",
    "        \"Product\",\n",
    "        \"GPE\",\n",
    "    ]\n",
    ")\n",
    "non_ent_columns = sorted([\"Cardinal\", \"Date\", \"Money\", \"Percent\", \"Quantity\", \"Time\"])\n",
    "columns_to_keep = ent_columns + non_ent_columns + [\"Average\"]\n",
    "\n",
    "_df = _df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = _df.T\n",
    "# iidx = pd.MultiIndex.from_product([ent_columns, non_ent_columns, [\"Average\"]], names=[\"Entity\", \"Non-Entity\", \"\"])\n",
    "iidx = pd.MultiIndex.from_arrays(\n",
    "    [\n",
    "        [\"Entities\"] * len(ent_columns)\n",
    "        + [\"Non-Entities\"] * len(non_ent_columns)\n",
    "        + [\"Average\"],\n",
    "        ent_columns + non_ent_columns + [\"Average\"],\n",
    "    ]\n",
    ")\n",
    "table.index = iidx\n",
    "\n",
    "mdl_names = [\"Large 0.1.0\", \"Medium 0.1.0\", \"Small 0.1.0\"]\n",
    "header = pd.MultiIndex.from_arrays(\n",
    "    [[\"Fine-grained Models\"] * len(mdl_names), mdl_names]\n",
    ")\n",
    "table.columns = header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to latex using styler\n",
    "style = table.style.format_index(escape=\"latex\", axis=1).format_index(\n",
    "    escape=\"latex\", axis=0\n",
    ")\n",
    "\n",
    "\n",
    "# highlight the maximum\n",
    "def italicize_second_max(s: pd.Series) -> list:\n",
    "    \"\"\"Italicize the second maximum in a Series.\"\"\"\n",
    "    is_second_max = s == s.sort_values(ascending=False).iloc[1]\n",
    "    # check if the second maximum is the same as the maximum\n",
    "    same_as_max = s == s.max()\n",
    "\n",
    "    if same_as_max.sum() > 1:\n",
    "        # if there are more than one maximum, don't italicize\n",
    "        return [\"font-style: normal\" for v in is_second_max]\n",
    "    return [\"font-style: italic\" if v else \"\" for v in is_second_max]\n",
    "\n",
    "\n",
    "style = style.apply(highlight_max, axis=1)\n",
    "# style = style.apply(underline_second_max, axis=1)\n",
    "style = style.apply(italicize_second_max, axis=1)\n",
    "\n",
    "# apply the CSS style\n",
    "super_header_style = [\n",
    "    {\"selector\": \".level0\", \"props\": [(\"text-align\", \"center\")]},\n",
    "    {\"selector\": \".col_heading\", \"props\": [(\"text-align\", \"center\")]},\n",
    "]\n",
    "style = style.set_table_styles(super_header_style)\n",
    "\n",
    "\n",
    "# add caption\n",
    "caption = \"F1 score with 95% confidence interval calculated using bootstrapping with 100 samples.\"\n",
    "style = style.set_caption(caption)\n",
    "style\n",
    "\n",
    "# latex = style.to_latex(\n",
    "#         hrules=True,\n",
    "#         convert_css=True,\n",
    "#     )\n",
    "\n",
    "# print(latex)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization\n",
    "To examine model generalization, we utilize the [DANSK](https://huggingface.co/datasets/chcaa/DANSK) dataset {cite}`missing`. This dataset is annotated across many different domains including fiction, web content, social media, wikis, news, legal and conversational data. The original dataset includes annotations corresponding to the ontonotes standard (see [getting started](https://centre-for-humanities-computing.github.io/DaCy/tutorials/basic.html#fine-grained-ner) for the full list). To test the generalization we here convert the annotations to the CoNLL-2003 format using the labels `Person`, `Location`, `Organization`. As CoNLL-2003, `Location` includes cities, roads, mountains, abstract places, specific buildings, and meeting points. Thus the `GPE` (geo-political entity) were converted to `Location`. The `MISC` category in CoNLL-2003 is a diverse category meant to denote all names not in other categories (encapsulating both e.g. events and adjectives such as ”2004 World Cup” and ”Italian”), and is therefore not included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_conll_2003(\n",
    "    examples,\n",
    "    mapping={\"PERSON\": \"PER\", \"GPE\": \"LOC\", \"LOCATION\": \"LOC\", \"ORGANIZATION\": \"ORG\", \"PER\": \"PER\", \"LOC\": \"LOC\", \"ORG\": \"ORG\"},\n",
    ") -> list:\n",
    "    \n",
    "    def doc_to_conll_2003(doc):\n",
    "        ents = doc.ents\n",
    "        ents = [e for e in ents if e.label_ in mapping]\n",
    "        for ent in ents:\n",
    "            ent.label_ = mapping[ent.label_]\n",
    "        doc.ents = ents\n",
    "        return doc\n",
    "\n",
    "    for example in examples:\n",
    "        example.y = doc_to_conll_2003(example.y)\n",
    "        example.x = doc_to_conll_2003(example.x)\n",
    "    return examples\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mdl_name in dansk:\n",
    "    examples = dansk[mdl_name][\"test\"][\"examples\"]\n",
    "    examples += dansk[mdl_name][\"dev\"][\"examples\"]\n",
    "    examples += dansk[mdl_name][\"train\"][\"examples\"]\n",
    "\n",
    "    examples = convert_to_conll_2003(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from performance_testing_utils.generalization_utils import (\n",
    "    dansk,\n",
    "    convert_to_conll_2003,\n",
    "    MDL_GETTER_DICT,\n",
    "    evaluate_generalization,\n",
    "    create_generation_viz,\n",
    ")\n",
    "\n",
    "train, dev, test = dansk()\n",
    "convert_to_conll_2003(train)\n",
    "convert_to_conll_2003(dev)\n",
    "convert_to_conll_2003(test)\n",
    "\n",
    "dataset = train + dev + test\n",
    "\n",
    "assert set([e.label_ for doc in dataset for e in doc.ents]) == set(\n",
    "    [\"PER\", \"LOC\", \"ORG\"]\n",
    ")\n",
    "\n",
    "save_folder = Path(\"performance_tables/ner\")\n",
    "save_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "tables = []\n",
    "# create domains datasets\n",
    "domains = {}\n",
    "for doc in dataset:\n",
    "    domain = doc._.meta[\"dagw_domain\"]\n",
    "    if domain not in domains:\n",
    "        domains[domain] = []\n",
    "    domains[domain].append(doc)\n",
    "\n",
    "for mdl, getter in MDL_GETTER_DICT.items():\n",
    "    mdl_name = mdl.replace(\"/\", \"_\")\n",
    "    save_path = save_folder / f\"{mdl_name}_generalization.csv\"\n",
    "    if not save_path.exists():\n",
    "        nlp = getter()\n",
    "        result_df = evaluate_generalization(\n",
    "            mdl_name=mdl, mdl=nlp, domains_dataset_dict=domains\n",
    "        )\n",
    "        result_df.to_csv(save_path, index=False)\n",
    "    else:\n",
    "        print(f\"- {mdl} already exists, loading in dataframe\")\n",
    "    result_df = pd.read_csv(\n",
    "        save_path\n",
    "    )  # always load in dataframe to ensure the same representation\n",
    "    tables.append(result_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
